title: NPFL114, Lecture 4
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Convolutional Neural Networks

## Milan Straka

### March 25, 2019

---
section: Gradient Clipping
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

~~~
We have already discussed two factors influencing it on the previous lecture:
- saturating non-linearities,
- parameter initialization strategies.

~~~
Another prominent method for dealing with slow or diverging training
is _gradient clipping_.

---
# Convergence ‚Äì Gradient Clipping

![w=100%](exploding_gradient.pdf)

---
# Convergence ‚Äì Gradient Clipping

![w=50%,h=center](gradient_clipping.pdf)

Using a given maximum norm, we may _clip_ the gradient.

~~~
$$g ‚Üê \begin{cases}
  g & \textrm{ if }||g|| ‚â§ c \\
  c \frac{g}{||g||} & \textrm{ if }||g|| > c
\end{cases}$$

~~~
The clipping can be per weight (`clipvalue` of `tf.keras.optimizers.Optimizer`), per variable
or for the gradient as a whole (`clipnorm` of `tf.keras.optimizers.Optimizer`).

---
section: Convolution
class: middle, center
# Going Deeper

# Going Deeper

---
# Convolutional Networks

Consider data with some structure (temporal data, speech, images, ‚Ä¶).

~~~
Unlike densely connected layers, we might want:
~~~
- local interactions;
~~~
- parameter sharing (equal response everywhere);
~~~
- shift invariance.

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $x * w$ is defined as
$$(x * w)(t) = ‚à´x(a)w(t - a)\d a.$$

~~~
For vectors, we have
$$(‚Üíx * ‚Üíw)_t = ‚àë\nolimits_i x_i w_{t-i}.$$

~~~
Convolution operation can be generalized to two dimensions by
$$(‚áâI * ‚áâK)_{i, j} = ‚àë\nolimits_{m, n} ‚áâI_{m, n} ‚áâK_{i-m, j-n}.$$

~~~
Closely related is _cross-corellation_, where $K$ is flipped:
$$S_{i, j} = ‚àë\nolimits_{m, n} ‚áâI_{i+m, j+n} ‚áâK_{m, n}.$$

---
# Convolution

![w=53%,h=center](convolution_all.pdf)

---
# Convolutional Networks

![w=73%,h=center](convolution.png)

---
section: CNNs
# Convolution Layer

The $K$ is usually called a _kernel_ or a _filter_, and we generally apply
several of them at the same time.

~~~
Consider an input image with $C$ channels. The convolution layer with
$F$ filters of width $W$, height $H$ and stride $S$ produces an output with $F$ channels,
is parametrized by a kernel $‚á∂K$ of total size $W √ó H √ó C √ó F$ and is computed as
$$(‚á∂I * ‚á∂K)_{i, j, k} = ‚àë_{m, n, o} ‚á∂I_{i‚ãÖS + m, j‚ãÖS + n, o} ‚á∂K_{m, n, o, k}.$$

~~~
We can consider the kernel to be composed of $F$ independent kernels, one for
every output channel.

~~~
Note that while only local interactions are performed in the image spacial dimensions
(width and height), we combine input channels in a fully connected manner.

---
# Convolution Layer

There are multiple padding schemes, most common are:
- `valid`: Only use valid pixels, which causes the result to be smaller than the input.
- `same`: Pad original image with zero pixels so that the result is exactly
  the size of the input.

~~~
There are two prevalent image formats (called `data_format` in TensorFlow):
- `channels_last`: The dimensions of the 4-dimensional image tensor are batch,
  height, width, and channels.

  The original TensorFlow format, faster on CPU.

~~~
- `channels_first`: The dimensions of the 4-dimensional image tensor are batch,
  channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

~~~
TensorFlow has been implementing an approach that will convert data format
to `channels_first` automatically depending on the backend.

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling (minor translation invariance)
- Average pooling

![w=60%,h=center](pooling.pdf)

---
section: AlexNet
# High-level CNN Architecture

We repeatedly use the following block:
1. Convolution operation
2. Non-linear activation (usually ReLU)
3. Pooling

![w=90%,h=center](cnn.jpg)

---
# AlexNet ‚Äì 2012 (16.4% error)

![w=100%](alexnet.pdf)

---
# AlexNet ‚Äì 2012 (16.4% error)

Training details:
- 2 GPUs for 5-6 days

~~~
- SGD with batch size 128, momentum 0.9, weight decay 0.0005

~~~
- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

~~~
- ReLU non-linearities

~~~
- dropout with rate 0.5 on fully-connected layers

~~~
- data augmentation using translations and horizontal reflections (choosing random
  $224 √ó 224$ patches from $256 √ó 256$ images)
~~~
  - during inference, 10 patches are used (four corner patches and a center
    patch, as well as their reflections)

---
# AlexNet ‚Äì ReLU vs tanh

![w=60%,h=center](relu_vs_tanh.pdf)

---
# LeNet ‚Äì 1998

AlexNet built on already existing CNN architectures, mostly on LeNet, which
achieved 0.8% test error on MNIST.

![w=100%,mh=80%,v=middle](lenet.pdf)

---
class: middle
# Similarities in V1 and CNNs

![w=100%](gabor_functions.pdf)
The primary visual cortex recognizes Gabor functions.

---
# Similarities in V1 and CNNs

![w=90%,h=center](first_convolutional_layer.pdf)
Similar functions are recognized in the first layer of a CNN.

---
section: Deep Prior
# CNNs as Regularizers ‚Äì Deep Prior

![w=50%,h=center](deep_prior_superresolution.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=82%,h=center](deep_prior_inpainting.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=100%,v=middle](deep_prior_inpainting_diversity.jpg)

---
# CNNs as Regularizers ‚Äì Deep Prior

![w=95%,h=center](deep_prior_inpainting_architecture.jpg)

[Deep Prior paper website with supplementary material](https://dmitryulyanov.github.io/deep_image_prior)

---
section: VGG
# VGG ‚Äì 2014 (6.8% error)

![w=48%,f=left](vgg_architecture.pdf)

![w=70%,mw=50%,h=center](inception3_conv5.png)
![w=95%,mw=50%,h=center,mh=40%,v=middle](vgg_parameters.pdf)

---
# VGG ‚Äì 2014 (6.8% error)

![w=100%,v=middle](vgg_inception_results.pdf)

---
section: Inception
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=100%](inception_block.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=90%,h=center](inception_block_reduction.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=93%,h=center](inception_architecture.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![w=10%,h=center](inception_graph.pdf)
Also note the two auxiliary classifiers (they have weight 0.3).

---
section: BatchNorm
# Batch Normalization

_Internal covariate shift_ refers to the change in the distributions
of hidden node activations due to the updates of network parameters
during training.

Let $‚Üíx = (x_1, \ldots, x_d)$ be $d$-dimensional input. We would like to
normalize each dimension as
$$xÃÇ_i = \frac{x_i - ùîº[x_i]}{\sqrt{\Var[x_i]}}.$$
Furthermore, it may be advantageous to learn suitable scale $Œ≥_i$ and shift $Œ≤_i$ to
produce normalized value
$$y_i = Œ≥_i xÃÇ_i + Œ≤_i.$$

---
# Batch Normalization

Consider a mini-batch of $m$ examples $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$.

_Batch normalizing transform_ of the mini-batch is the following transformation.

<div class="algorithm">

**Inputs**: Mini-batch $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$, $Œµ ‚àà ‚Ñù$<br>
**Outputs**: Normalized batch $(‚Üíy^{(1)}, \ldots, ‚Üíy^{(m)})$
- $‚ÜíŒº ‚Üê \frac{1}{m} ‚àë_{i = 1}^m ‚Üíx^{(i)}$
- $‚ÜíœÉ^2 ‚Üê \frac{1}{m} ‚àë_{i = 1}^m (‚Üíx^{(i)} - Œº)^2$
- $\hat‚Üíx^{(i)} ‚Üê (‚Üíx^{(i)} - ‚ÜíŒº) / \sqrt{œÉ^2 + Œµ}$
- $‚Üíy^{(i)} ‚Üê ‚ÜíŒ≥ \hat‚Üíx^{(i)} + ‚ÜíŒ≤$
</div>

~~~
Batch normalization is commonly added just before a nonlinearity. Therefore, we
replace $f(‚áâW‚Üíx + ‚Üíb)$ by $f(\textit{BN}(‚áâW‚Üíx))$.

~~~
During inference, $‚ÜíŒº$ and $‚ÜíœÉ^2$ are fixed. They are either precomputed
after training on the whole training data, or an exponential moving average is
updated during training.

---
# Batch Normalization

When a batch normalization is used on a fully connected layer, each neuron
is normalized individually across the minibatch.

~~~
However, for convolutional networks we would like the normalization to
honour their properties, most notably the shift invariance. We therefore
normalize each channel across not only the minibatch, but also across
all corresponding spacial/temporal locations.

![w=70%,h=center](batch_normalization_variants.pdf)

---
# Inception with BatchNorm (4.8% error)

![w=100%,v=middle](inception_batchnorm.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=90%,mw=61%,h=center](inception3_conv5.png)
![w=90%,mw=37%,h=center](inception3_conv3.png)

---
class: middle
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=32%](inception3_inception_a.pdf)
![w=32%](inception3_inception_b.pdf)
![w=32%](inception3_inception_c.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=55%,h=center](inception3_architecture.pdf)

---
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![w=60%,h=center](inception3_ablation.pdf)

---
section: ResNet
# ResNet ‚Äì 2015 (3.6% error)

![w=95%,h=center](resnet_depth_effect.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=90%,h=center](resnet_block.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%](resnet_block_reduced.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%](resnet_architecture.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=42%,mw=50%,h=center,f=left](resnet_overall.pdf)

~~~
The residual connections cannot be applied directly when
number of channels increase.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1√ó1$ convolution is used on the projections to match the
required number of channels.

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%,v=middle](resnet_residuals.pdf)

---
# ResNet ‚Äì 2015 (3.6% error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![w=49%](resnet_validation.pdf)
![w=49%](resnet_testing.pdf)

---
# Main Takeaways

- Convolutions can provide

  - local interactions in spacial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3√ó3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double number of channels.

~~~
- Final fully connected layers are not needed, global average pooling
  is usually enough.

~~~
- Batch normalization is a great regularization method for CNNs.
