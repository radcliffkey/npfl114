title: NPFL114, Lecture 3
class: title, langtech, cc-by-nc-sa

# Training Neural Networks II

## Milan Straka

### March 18, 2019

---
section: NN Training
# Putting It All Together

Let us have a dataset with a training, validation and test sets, each containing
examples $(→x, y)$. Depending on $y$, consider one of the following output
activation functions:
$$\begin{cases}
  \textrm{none} & \textrm{ if } y ∈ ℝ\\
  σ & \textrm{ if } y \textrm{ is a probability of an outcome}\\
  \softmax & \textrm{ if } y \textrm{ is a gold class}
\end{cases}$$

If $→x ∈ ℝ^d$, we can use a neural network with an input layer of size $d$,
hidden layer of size $h$ with a non-linear activation function, and an output
layer of size $o$ (either 1 or number of classification classes) with the
mentioned output function.

---
# Putting It All Together
![w=90%,mw=50%,f=left](../01/neural_network.svg)

~~~
We have
$$h_i = f^{(1)}\left(∑_j ⇉W^{(1)}_{i,j} x_j + b^{(1)}_i\right)$$
where
- $⇉W^{(1)} ∈ ℝ^{h×d}$ is a matrix of _weights_,
- $→b^{(1)} ∈ ℝ^h$ is a vector of _biases_,
- $f^{(1)}$ is an activation function.

~~~
The weights are sometimes also called a _kernel_.

~~~
The biases define general behaviour in case of zero/very small input.

~~~
Transformations of type $wx + b$ are called _affine_ instead of _linear_.

--- -----
Similarly
$$o_i = f^{(2)}\left(∑_j ⇉W^{(2)}_{i,j} h_j + b^{(2)}_i\right)$$
with
- $⇉W^{(2)} ∈ ℝ^{o×h}$ another matrix of weights,
- $→b^{(2)} ∈ ℝ^o$ another vector of biases,
- $f^{(2)}$ being an output activation function.

---
# Putting It All Together

The parameters of the model are therefore $⇉W^{(1)}, ⇉W^{(2)}, →b^{(1)}, →b^{(2)}$ of total size
$d×h + h×o + h + o$.

~~~
To train the network, we repeatedly sample $m$ training examples and perform
SGD (or any its adaptive variant), updating the parameters to minimize the
loss.

$$θ_i ← θ_i - α\frac{∂L}{∂θ_i}$$

~~~
We set the hyperparameters (size of the hidden layer, hidden layer activation
function, learning rate, …) using performance on the validation set and evaluate
generalization error on the test set.

---
# Practical Issues

- Processing all input in _batches_.

~~~
- Vector representation of the network.

~~~
  Considering $h_i = f^{(1)}\left(∑_j ⇉W^{(1)}_{i,j} x_j + b^{(1)}_i\right)$,
  we can write
  $$→h = f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)$$
  $$→o = f^{(2)}\left(⇉W^{(2)} →h + →b^{(2)}\right) = f^{(2)}\left(⇉W^{(2)} \left(f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)\right) + →b^{(2)}\right)$$

~~~
  The derivatives
  $$\frac{∂f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)}{∂→x},
  \frac{∂f^{(1)}\left(⇉W^{(1)} →x + →b^{(1)}\right)}{∂⇉W^{(1)}}$$
  are then matrices (called _Jacobians_) or even higher-dimensional tensors.

---
class: center, middle
# Computation Graph

![w=35%](../01/neural_network.svg)
→
![w=26%](computation_graph.svg)

---
class: tablefull
# High Level Overview

|                 | Classical ('90s) | Deep Learning |
|-----------------|------------------|---------------|
|Architecture     | $\vdots\,\,\vdots\,\,\vdots$ | $\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots$ &nbsp; CNN, RNN, Transformer, VAE, GAN, …
|Activation func. | $\tanh, σ$    | $\tanh$, ReLU, PReLU, ELU, SELU, Swish, …
|Output function  | none, $σ$     | none, $σ$, $\softmax$
|Loss function    | MSE           | NLL (or cross-entropy or KL-divergence)
|Optimization     | SGD, momentum | SGD, RMSProp, Adam, …
|Regularization   | L2, L1        | L2, Dropout, Label smoothing, BatchNorm, LayerNorm, …

---
section: Loss Derivatives
# Derivative of MSE Loss

Given the MSE loss of
$$L = \big(y - ŷ(→x; →θ)\big)^2 = \big(ŷ(→x; →θ) - y\big)^2,$$
the derivative with respect to $ŷ$ is simply:
$$\frac{∂L}{ŷ(→x; →θ)} = 2\big(ŷ(→x; →θ) - y\big).$$

---
# Derivative of Softmax MLE Loss

![w=38%,h=center](softmax.svg)

Let us have a softmax output layer with
$$o_i = \frac{e^{z_i}}{∑_j e^{z_j}}.$$

---
# Derivative of Softmax MLE Loss

Consider now the MLE estimation. The loss for gold class index $\textit{gold}$ is then
$$L(\softmax(→z), \textit{gold}) = - \log o_\textit{gold}.$$

~~~
The derivation of the loss with respect to $→z$ is then
$$\begin{aligned}
\frac{∂L}{∂z_i} = \frac{∂}{∂z_i} \left[-\log \frac{e^{z_\textit{gold}}}{∑_j e^{z_j}}\right] 
                =& -\frac{∂z_\textit{gold}}{∂z_i} + \frac{∂\log(∑_j e^{z_j})}{∂z_i} \\
                =& -[\textit{gold} = i] + \frac{1}{∑_j e^{z_j}} e^{z_i} \\
                =& -[\textit{gold} = i] + o_i.
\end{aligned}$$

~~~
Therefore, $\frac{∂L}{∂→z} = - →1_\textit{gold} + →o$, where $→1_\textit{gold}$
is 1 at index $\textit{gold}$ and 0 otherwise.

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_sparse.pdf)

---
# Derivative of Softmax and Sigmoid MLE Losses

In the previous case, the gold distribution was _sparse_, with only one
target probability being 1.

In the case of general gold distribution $→g$, we have
$$L(\softmax(→z), →g) = -∑_i g_i \log o_i.$$
Repeating the previous procedure for each target probability, we obtain
$$\frac{∂L}{∂→z} = -→g + →o.$$

~~~
## Sigmoid

Analogously, for $o = σ(z)$ we get $\frac{∂L}{∂z} = -g + o$,
where $g$ is the target gold probability.

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_full.pdf)

---
section: Regularization
# Regularization

As already mentioned, regularization is any change in the machine learning
algorithm that is designed to reduce generalization error but not necessarily
its training error.

Regularization is usually needed only if training error and generalization error
are different. That is often not the case if we process each training example
only once. Generally the more training data, the better generalization
performance.

- Early stopping

- L2, L1 regularization

- Dataset augmentation

- Ensembling

- Dropout

- Label smoothing

---
# Regularization – Early Stopping

![w=100%](early_stopping.pdf)

---
section: L2
# L2 Regularization

We prefer models with parameters small under L2 metric.

The L2 regularization, also called _weight decay_, _Tikhonov regularization_ or
_ridge regression_ therefore minimizes
$$J̃(→θ; 𝕏) = J(→θ; 𝕏) + λ ||→θ||_2^2$$
for a suitable (usually very small) $λ$.

During the parameter update of SGD, we get
$$θ_i ← θ_i - α\frac{∂J}{∂θ_i} - 2αλθ_i$$

---
# L2 Regularization

![w=50%,h=center](l2_regularization.pdf)

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to utilize Bayesian inference.

~~~
With MLE we have
$$→θ_\mathrm{MLE} = \argmax_→θ p(𝕏; →θ).$$

~~~
Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$→θ_\mathrm{MAP} = \argmax_→θ p(→θ; 𝕏)$$

~~~
Using Bayes' theorem
$$p(→θ; 𝕏) = p(𝕏; →θ) p(→θ) / p(𝕏),$$
we get
$$→θ_\mathrm{MAP} = \argmax_→θ p(𝕏; →θ)p(→θ).$$

---
# L2 Regularization as MAP

The $p(→θ)$ are prior probabilities of the parameter values (our _preference_).

One possibility for such a prior is $\N(→θ; 0, σ^2)$.

Then
$$\begin{aligned}
→θ_\mathrm{MAP} &= \argmax_→θ p(𝕏; →θ)p(→θ) \\
                &= \argmax_→θ ∏\nolimits_{i=1}^m p(→x^{(i)}; →θ)p(→θ) \\
                &= \argmin_→θ ∑\nolimits_{i=1}^m -\log p(→x^{(i)}; →θ) - \log p(→θ) \\
\end{aligned}$$

By substituting the probability of the Gaussian prior, we get
$$→θ_\mathrm{MAP} = \argmin_→θ ∑_{i=1}^m -\log p(→x^{(i)}; →θ) {\color{gray} + \frac{1}{2} \log(2πσ^2)} + \frac{→θ^2}{2σ^2}$$

---
# L1 Regularization

Similar to L2 regularization, but we prefer low L1 metric of parameters. We
therefore minimize
$$J̃(→θ; 𝕏) = J(→θ; 𝕏) + λ ||→θ||_1$$

The corresponding SGD update is then 
$$θ_i ← θ_i - α\frac{∂J}{∂θ_i} - \operatorname{sign}(θ_i)αλ.$$

---
# Regularization – Dataset Augmentation

For some data, it is cheap to generate slightly modified examples.

- Image processing: translations, horizontal flips, scaling, rotations, color
  adjustments, …

~~~
  - Mixup (appeared in 2017)

  ![w=25%,h=center](mixup.pdf)

~~~
- Speech recognition: noise, frequency change, …

~~~
- More difficult for discrete domains like text.

---
section: Ensembling
# Regularization – Ensembling

Ensembling (also called _model averaging_ or in some contexts _bagging_) is a general technique
for reducing generalization error by combining several models. The models are
usually combined by averaging their outputs (either distributions or output
values in case of a regression).

~~~
The main idea behind ensembling it that if models have uncorrelated
(independent) errors, then by averaging model outputs the errors will cancel
out.

~~~
However, ensembling usually has high performance requirements.

---
# Regularization – Ensembling

There are many possibilities how to train the models to average:

- Generate different datasets by sampling with replacement (bagging).

![w=50%,h=center](ensembling.pdf)

~~~
- Use random different initialization.

~~~
- Average models from last hours/days of training.

---
section: Dropout
# Regularization – Dropout

How to design good universal features?

- In reproduction, evolution is achieved using gene swapping. The genes must not
  be just good with combination with other genes, they need to be universally
  good.

~~~
Idea of _dropout_ by (Srivastava et al., 2014), in preprint since 2012.

~~~
When applying dropout to a layer, we drop each neuron independently with
a probability of $p$ (usually called _dropout rate_). To the rest of the
network, the dropped neurons have value of zero.

~~~
![w=60%,h=center](network_with_dropout.png)

---
# Regularization – Dropout

Dropout is performed only when training, during inference no nodes are
dropped. However, in that case we need to _scale the activations down_
by a factor of $1-p$ to account for more neurons than usual.

![w=75%,h=center,mh=80%,v=middle](dropout_inference.pdf)

---
# Regularization – Dropout

Alternatively, we might _scale the activations up_ during training by a factor
of $1/(1-p)$.

![w=75%,h=center,mh=90%,v=middle](dropout_inference_2.pdf)

---
# Regularization – Dropout as Ensembling

![w=50%,h=center](dropout_ensembling.pdf)

---
# Regularization – Dropout Effect
![w=85%,h=center](dropout_features.pdf)

---
# Regularization – Dropout Implementation

```python
def dropout(input, rate=0.5, training=False):
    def do_inference():
        return tf.identity(input)

    def do_train():
        random_noise = tf.random_uniform(tf.shape(input))
        mask = tf.cast(tf.less(random_noise, rate), tf.float32)
        return input * mask / (1 - rate)

    if training == True:
        return do_train()
    if training == False:
        return do_inference()
    return tf.cond(training, do_train, do_inference)
```

---
section: Label Smoothing
# Regularization – Label Smoothing

Problem with softmax MLE loss is that it is _never satisfied_, always pushing
the gold label probability higher (but it saturates near 1).

~~~
This behaviour can be responsible for overfitting, because the network is always
commanded to respond more strongly to the training examples, not respecting
similarity of different training examples.

~~~
Ideally, we would like a full (non-sparse) categorical distribution of classes
for training examples, but that is usually not available.

~~~
We can at least a simple smoothing technique, called _label smoothing_, which
allocates some small probability volume $α$ uniformly for all possible classes.

~~~
The target distribution is then
$$(1-α)→1_\textit{gold} + α \frac{→1}{\textrm{number~of~classes}}.$$

---
# Regularization – Label Smoothing

![w=100%,v=middle](label_smoothing.pdf)

---
# Regularization – Good Defaults

When you need to regularize, then a good default strategy is to:

- use dropout on all hidden dense layers (not on the output layer), good default
  dropout rate is 0.5 (or use 0.3 if the model is underfitting);

~~~
- use L2 regularization for your convolutional networks;
~~~
- use label smoothing (start with 0.1);
~~~
- if you require best performance and have a lot of resources, also
  perform ensembling.

---
section: Convergence
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

There are _many_ factors influencing convergence and its speed, we now discuss
three of them.

---
# Convergence – Saturating Non-linearities

![w=100%,v=middle](tanh.png)

---
# Convergence – Parameter Initialization

Neural networks usually need random initialization to _break symmetry_.

- Biases are usually initialized to a constant value, usually 0.

~~~

- Weights are usually initialized to small random values, either with uniform or
  normal distribution.
   - The scale matters for deep networks!

   - Originally, people used $U\left[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}\right]$ distribution.

~~~
   - Xavier Glorot and Yoshua Bengio, 2010:
     _Understanding the difficulty of training deep feedforward neural networks_.

     The authors theoretically and experimentally show that a suitable way to
     initialize a $ℝ^{n×m}$ matrix is
     $$U\left[-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right].$$

---
# Convergence – Parameter Initialization

![w=62%,h=center](glorot_activations.pdf)

---
# Convergence – Parameter Initialization

![w=63%,h=center](glorot_gradients.pdf)

---
# Convergence – Gradient Clipping

![w=100%](exploding_gradient.pdf)

---
# Convergence – Gradient Clipping

![w=50%,h=center](gradient_clipping.pdf)

~~~
Using a given maximum norm, we may clip the gradient.
$$g ← \begin{cases}
  g & \textrm{ if }||g|| ≤ c \\
  c \frac{g}{||g||} & \textrm{ if }||g|| > c
\end{cases}$$

The clipping can be per weight (`clipvalue` of `tf.keras.optimizers.Optimizer`), per matrix
or for the gradient as a whole (`clipnorm` of `tf.keras.optimizers.Optimizer`).
