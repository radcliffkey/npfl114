title: NPFL114, Lecture 6
class: title, langtech, cc-by-nc-sa

# Convolutional Neural Networks III, Recurrent Neural Networks

## Milan Straka

### April 08, 2019

---
section: Refresh
# Fast R-CNN

![w=100%,v=middle](../05/fast_rcnn.jpg)

---
# Fast R-CNN

The bounding box is parametrized as follows. Let $x_r, y_r, w_r, h_r$ be
center coordinates and width and height of the RoI, and let $x, y, w, h$ be
parameters of the bounding box. We represent them as follows:
$$\begin{aligned}
t_x &= (x - x_r)/w_r, & t_y &= (y - y_r)/h_r \\
t_w &= \log (w/w_r), & t_h &= \log (h/h_r)
\end{aligned}$$

Usually a $\textrm{smooth}_{L_1}$ loss, or _Huber loss_, is employed for bounding box parameters
$$\textrm{smooth}_{L_1}(x) = \begin{cases}
  0.5x^2    & \textrm{if }|x| < 1 \\
  |x| - 0.5 & \textrm{otherwise}
\end{cases}$$

The complete loss is then
$$L(ĉ, t̂, c, t) = L_\textrm{cls}(ĉ, c) + λ[c ≥ 1]
  ∑_{i ∈ \lbrace \mathrm{x, y, w, h}\rbrace} \textrm{smooth}_{L_1}(t̂_i - t_i).$$

---
# Fast R-CNN

## Intersection over union
For two bounding boxes (or two masks) the _intersection over union_ (_IoU_)
is a ration of the intersection of the boxes (or masks) and the union
of the boxes (or masks).

## Choosing RoIs for training
During training, we use $2$ images with $64$ RoIs each. The RoIs are selected
so that $25\%$ have intersection over union (IoU) overlap with ground-truth
boxes at least 0.5; the others are chosen to have the IoU in range $[0.1, 0.5)$.

## Choosing RoIs during inference
Single object can be found in multiple RoIs. To choose the most salient one,
we perform _non-maximum suppression_ -- we ignore RoIs which have an overlap
with a higher scoring RoI of the same type, where the IoU is larger than a given
threshold (usually, 0.3 is used).

---
# Object Detection Evaluation

## Average Precision
Evaluation is performed using _Average Precision_ (_AP_).

We assume all bounding boxes (or masks) produced by a system have confidence
values which can be used to rank them. Then, for a single class, we take the
boxes (or masks) in the order of the ranks and generate precision/recall curve,
considering a bounding box correct if it has IoU at least 0.5 with any
ground-truth box.
We define _AP_ as an average of precisions for recall levels $0, 0.1, 0.2,
\ldots, 1$.

~~~
![w=55%,mw=50%,h=center](../05/precision_recall_person.pdf)![w=55%,mw=50%,h=center](../05/precision_recall_bottle.pdf)

---
# Faster R-CNN

For Fast R-CNN, the most time consuming part is generating the RoIs.

Therefore, Faster R-CNN jointly generates _regions of interest_ using
a _region proposal network_ and performs object detection.

![w=39%,h=center](../05/faster_rcnn_architecture.jpg)

---
# Mask R-CNN

![w=100%,v=middle](../05/mask_rcnn_architecture.jpg)

---
# Mask R-CNN

## RoIAlign

More precise alignment is required for the RoI in order to predict the masks.
Therefore, instead of max-pooling used in the RoI pooling, RoIAlign with
bilinear interpolation is used.

![w=38%,h=center](../05/mask_rcnn_roialign.pdf)

---
# Mask R-CNN

Masks are predicted in a third branch of the object detector.

- Usually higher resolution is needed ($14×14$ instead of $7×7$).
- The masks are predicted for each class separately.
- The masks are predicted using convolutions instead of fully connected layers.

![w=100%](../05/mask_rcnn_heads.pdf)

---
section: ImageSegmentation
# Mask R-CNN

![w=100%,v=middle](../05/mask_rcnn_ablation.pdf)

---
# Mask R-CNN – Human Pose Estimation

![w=80%,h=center](../01/human_pose_estimation.pdf)

~~~
- Testing applicability of Mask R-CNN architecture.

- Keypoints (e.g., left shoulder, right elbow, …) are detected
  as independent one-hot masks of size $56×56$ with $\softmax$ output function.

~~~
![w=70%,h=center](mask_rcnn_hpe_performance.pdf)

---
section: FPN
# Feature Pyramid Networks

![w=85%,h=center](fpn_overview.pdf)

---
# Feature Pyramid Networks

![w=62%,h=center](fpn_architecture.pdf)

---
# Feature Pyramid Networks

![w=56%,h=center](fpn_architecture_detailed.pdf)

---
# Feature Pyramid Networks

![w=100%,v=middle](fpn_results.pdf)

---
section: FocalLoss
# Focal Loss

![w=55%,f=right](focal_loss_graph.pdf)

For single-stage object detection architectures, _class imbalance_ has been
identified as the main issue preventing to obtain performance comparable to
two-stage detectors. In a single-stage detector, there can be tens of thousands
of anchors, with only dozens of useful training examples.

~~~
Cross-entropy loss is computed as
$$𝓛_\textrm{cross-entropy} = -\log p_\textrm{model}(y | x).$$

~~~
Focal-loss (loss focused on hard examples) is proposed as
$$𝓛_\textrm{focal-loss} = -(1 - p_\textrm{model}(y | x))^γ ⋅ \log p_\textrm{model}(y | x).$$

---
# Focal Loss

![w=100%,v=middle](focal_loss_cdf.pdf)

---
# RetinaNet

![w=100%](retinanet.pdf)
![w=68%,h=center](retinanet_results.pdf)

---
section: TransferLearning
# Transfer Learning

In many situations, we would like to utilize a model trained on a different
dataset – generally, this cross-dataset usage is called _transfer learning_.

~~~
In image processing, models trained on ImageNet are frequently used as general
**feature extraction models**.

~~~
The easiest scenario is to take a ImageNet model, drop the last classification
layer, and use the result of the global average pooling as image features.
The ImageNet model is not modified during training.

~~~
For efficiency, we may precompute the image features **once** and reuse it later
many times.

---
# Transfer Learning – Finetuning

After we have successfully trained a network employing an ImageNet model,
we may improve performance further by _finetuning_ – training the full network
including the ImageNet model, allowing the feature extraction to adapt to the
current dataset.

~~~
- The laters after the ImageNet models **must** be already trained to
  convergence.

~~~
- Usually a smaller learning rate is necessary (for example one tenth of the
  original one, i.e., 0.0001 for Adam).

~~~
- We have to think about batch normalization, data augmentation or other
  regularization techniques.

---
section: GroupNorm
# Normalization

## Batch Normalization

Neuron value is normalized across the minibatch, and in case of CNN also across
all positions.

~~~
## Layer Normalization

Neuron value is normalized across the layer.

~~~
![w=100%](normalizations.pdf)

---
# Group Normalization

Group Normalization is analogous to Layer normalization, but the channels are
normalized in groups (by default, $G=32$).

![w=40%,h=center](normalizations.pdf)

~~~
![w=40%,h=center](group_norm.pdf)

---
# Group Normalization

![w=78%,h=center](group_norm_vs_batch_norm.pdf)

---
# Group Normalization

![w=65%,h=center](group_norm_coco.pdf)

---
section: RNN
class: middle, center
# Recurrent Neural Networks

# Recurrent Neural Networks

---
# Recurrent Neural Networks

## Single RNN cell

![w=17%,h=center](rnn_cell.pdf)

~~~

## Unrolled RNN cells

![w=60%,h=center](rnn_cell_unrolled.pdf)

---
# Basic RNN Cell

![w=50%,h=center,mh=55%](rnn_cell_basic.pdf)

~~~

Given an input $→x^{(t)}$ and previous state $→s^{(t-1)}$, the new state is computed as
$$→s^{(t)} = f(→s^{(t-1)}, →x^{(t)}; →θ).$$

~~~

One of the simplest possibilities is
$$→s^{(t)} = \tanh(⇉U→s^{(t-1)} + ⇉V→x^{(t)} + →b).$$

---
# Basic RNN Cell

Basic RNN cells suffer a lot from vanishing/exploding gradients (_the challenge
of long-term dependencies_).

~~~

If we simplify the recurrence of states to
$$→s^{(t)} = ⇉U→s^{(t-1)},$$
we get
$$→s^{(t)} = ⇉U^t→s^{(0)}.$$

~~~

If $U$ has eigenvalue decomposition of $⇉U = ⇉Q ⇉Λ ⇉Q^{-1}$, we get
$$→s^{(t)} = ⇉Q ⇉Λ^t ⇉Q^{-1} →s^{(0)}.$$
The main problem is that the _same_ function is iteratively applied many times.

~~~
Several more complex RNN cell variants have been proposed, which alleviate
this issue to some degree, namely **LSTM** and **GRU**.

---
# Basic RNN Applications

## Sequence Element Classification

Use outputs for individual elements.

![w=70%,h=center](rnn_cell_unrolled.pdf)

~~~
## Sequence Representation

Use state after processing the whole sequence (alternatively, take output of the
last element).

---
# Basic RNN Applications

## Sequence Prediction

During training, predict next sequence element.

![w=65%,h=center](sequence_prediction_training.pdf)

~~~
During inference, use predicted elements as further inputs.

![w=65%,h=center](sequence_prediction_inference.pdf)

---
section: LSTM
# Long Short-Term Memory

Hochreiter & Schmidhuber (1997) suggested that to enforce
_constant error flow_, we would like
$$f' = →1.$$

They propose to achieve that by a _constant error carrousel_.

![w=60%,h=center](lstm_cec.pdf)

---
# Long Short-Term Memory

They also propose an _input_ and _output_ gates which control the flow
of information into and out of the carrousel (_memory cell_ $→c_t$).

![w=40%,f=right](lstm_input_output_gates.pdf)

$$\begin{aligned}
  →i_t & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  →o_t & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  →c_t & ← →c_{t-1} + →i_t \cdot \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  →h_t & ← →o_t \cdot \tanh(→c_t)
\end{aligned}$$

---
# Long Short-Term Memory

Later in Gers, Schmidhuber & Cummins (1999) a possibility to _forget_
information from memory cell $→c_t$ was added.

![w=40%,f=right](lstm_input_output_forget_gates.pdf)

$$\begin{aligned}
  →i_t & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  →f_t & ← σ(⇉W^f →x_t + ⇉V^f →h_{t-1} + →b^f) \\
  →o_t & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  →c_t & ← →f_t \cdot →c_{t-1} + →i_t \cdot \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  →h_t & ← →o_t \cdot \tanh(→c_t)
\end{aligned}$$

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-SimpleRNN.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-chain.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-C-line.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-i.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-f.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-C.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-o.png)

---
section: GRU
# Gated Recurrent Unit

_Gated recurrent unit (GRU)_ was proposed by Cho et al. (2014) as
a simplification of LSTM. The main differences are
- no memory cell
- forgetting and updating tied together

![w=50%,h=center](gru.pdf)

---
# Gated Recurrent Unit

![w=50%,h=center](gru.pdf)

$$\begin{aligned}
  →r_t & ← σ(⇉W^r →x_t + ⇉V^r →h_{t-1} + →b^r) \\
  →u_t & ← σ(⇉W^u →x_t + ⇉V^u →h_{t-1} + →b^u) \\
  →ĥ_t & ← \tanh(⇉W^h →x_t + ⇉V^h (→r_t \cdot →h_{t-1}) + →b^h) \\
  →h_t & ← →u_t \cdot →h_{t-1} + (1 - →u_t) \cdot →ĥ_t
\end{aligned}$$

---
# Gated Recurrent Unit
![w=100%,v=middle](LSTM3-var-GRU.png)
